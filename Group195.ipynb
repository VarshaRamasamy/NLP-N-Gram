{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303143d8",
   "metadata": {},
   "source": [
    "# N-gram Language Modeling Assignment\n",
    "## Group Number: 195\n",
    "\n",
    "### Group Details\n",
    "- **Team Members:**\n",
    "\n",
    "  | Name | Email | Contribution |\n",
    "  |------|-------|--------------|\n",
    "  | VARSHA RAMASAMY | 2024ac05196@wilp.bits-pilani.ac.in | 100% |\n",
    "  | SHUBHA SHREE T | 2024ac05578@wilp.bits-pilani.ac.in | 100% |\n",
    "  | SHIVA SANKAR G P | 2024ac05899@wilp.bits-pilani.ac.in | 100% |\n",
    "  | SWARNALAXMI T | 2024ac05599@wilp.bits-pilani.ac.in | 100% |\n",
    "  | DEEPIKA KUMARI PANDAY | 2024ac05082@wilp.bits-pilani.ac.in | 100% |\n",
    "\n",
    "## Problem Statement\n",
    "Develop an N-gram language model to predict the probability of word sequences and generate text. The model will be trained on the English news corpus (100K sentences) and evaluated using perplexity and other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857092f-bb00-4d27-8b91-896827401a00",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "1. Load the English news corpus.\n",
    "2. Tokenize each sentence into words.\n",
    "3. Convert all tokens to lowercase.\n",
    "4. Build a vocabulary from the training data.\n",
    "5. Replace rare / unseen words with a special `<UNK>` token to handle OOV words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "208a1b66-6f14-4087-b044-db6a70060b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "NLTK data downloaded successfully!\n",
      "NLTK data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/shiva.gp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/shiva.gp/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "3b4d81fc-663c-4ec4-9d4a-cf700df91b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded successfully from: /Users/shiva.gp/Documents/SHIVA-PERSONAL/BITS_CLASS/SEM2/NaturalLanguageProcessing/Assignment/Assignment1/PROJECT/Question/eng_news_2016_100K-sentences.txt\n",
      "\n",
      "Dataset shape: (100000, 1)\n",
      "Total sentences: 100,000\n",
      "                                                    text\n",
      "0                          1\\t! 03alpe01 It's a rip off.\n",
      "1      2\\t05:36, UK, Wednesday 17 April 2013 US autho...\n",
      "2      3\\t$0.99 for first 4 weeks Subscribe Now Labor...\n",
      "3      4\\t$0.99 for first 4 weeks Subscribe Now Updat...\n",
      "4      5\\t100 YEARS AGO Jan. 29, 1910 It will be only...\n",
      "...                                                  ...\n",
      "99995  99996\\tZvizdic argued that such measures would...\n",
      "99996  99997\\tZwigoff: I didn't go into this film or ...\n",
      "99997  99998\\tZwizwai confirmed that there were attem...\n",
      "99998  99999\\tچشم انداز مذاکرات ایران و امریکا (بخش د...\n",
      "99999  100000\\t在大會上，女神為女性安全再次發聲，代表\"HeforShe\"運動發表消除校園暴...\n",
      "\n",
      "[100000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load text data from .txt file\n",
    "# Replace 'your_dataset.txt' with your actual file path\n",
    "file_path = \"/Users/shiva.gp/Documents/SHIVA-PERSONAL/BITS_CLASS/SEM2/NaturalLanguageProcessing/Assignment/Assignment1/PROJECT/Question/eng_news_2016_100K-sentences.txt\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Remove empty lines and strip whitespace\n",
    "sentences = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'text': sentences})\n",
    "\n",
    "print(f\"✓ Data loaded successfully from: {file_path}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Total sentences: {len(df):,}\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19571c92-4ef6-450e-811e-ebb33c126d3b",
   "metadata": {},
   "source": [
    "### Display Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "22039370-dbcb-46b7-9e7f-ed2acfcda769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 sentences from the dataset:\n",
      "======================================================================\n",
      "1. 1\t! 03alpe01 It's a rip off.\n",
      "2. 2\t05:36, UK, Wednesday 17 April 2013 US authorities have released the first pictures of the explosive devices used in the twin blasts which hit the Boston Marathon on Monday.\n",
      "3. 3\t$0.99 for first 4 weeks Subscribe Now Labor constraints and rising construction costs held back North Texas homebuilders in the most recent quarter.\n",
      "4. 4\t$0.99 for first 4 weeks Subscribe Now Updated at 10 p.m.: Revised to include the boy's name.\n",
      "5. 5\t100 YEARS AGO Jan. 29, 1910 It will be only a matter of time until the University of Arkansas will have a department of Journalism.\n",
      "Dataset Information:\n",
      "======================================================================\n",
      "Total sentences: 100,000\n",
      "Column names: ['text']\n",
      "\n",
      "Text Length Statistics:\n",
      "Average sentence length: 122.96 characters\n",
      "Minimum sentence length: 20 characters\n",
      "Maximum sentence length: 261 characters\n",
      "Median sentence length: 116 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 sentences from the dataset:\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(min(5, len(df))):\n",
    "    print(f\"{i+1}. {df['text'].iloc[i]}\")\n",
    "\n",
    "print(f\"Dataset Information:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total sentences: {len(df):,}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "\n",
    "# Calculate text length statistics\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"Average sentence length: {df['text_length'].mean():.2f} characters\")\n",
    "print(f\"Minimum sentence length: {df['text_length'].min()} characters\")\n",
    "print(f\"Maximum sentence length: {df['text_length'].max()} characters\")\n",
    "print(f\"Median sentence length: {df['text_length'].median():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c49ead-515c-45cb-989d-6d8dc0db6b97",
   "metadata": {},
   "source": [
    "## 1.1: Tokenize the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "e1273cd7-b6cd-4306-b585-1509330f38b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenization complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text_nltk(text):\n",
    "    \"\"\"\n",
    "    Tokenize text using NLTK's word_tokenize\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization\n",
    "df['tokens'] = df['text'].apply(tokenize_text_nltk)\n",
    "\n",
    "print(\"✓ Tokenization complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97eec3-befb-4328-91a3-2a583ab0fe7d",
   "metadata": {},
   "source": [
    "### Display Tokenization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "92d3f829-2180-4c6b-89df-bca52ba9d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 2,400,354\n",
      "Average tokens per sentence: 24.00\n",
      "Min tokens in a sentence: 4\n",
      "Max tokens in a sentence: 70\n",
      "Total tokens: 2,400,354\n",
      "Average tokens per sentence: 24.00\n",
      "Min tokens in a sentence: 4\n",
      "Max tokens in a sentence: 70\n",
      "\n",
      "Examples of tokenized text:\n",
      "======================================================================\n",
      "\n",
      "Sentence 1:\n",
      "Original: 1\t! 03alpe01 It's a rip off.\n",
      "Tokenized: ['1', '!', '03alpe01', 'It', \"'s\", 'a', 'rip', 'off', '.']\n",
      "Number of tokens: 9\n",
      "\n",
      "Sentence 2:\n",
      "Original: 2\t05:36, UK, Wednesday 17 April 2013 US authorities have released the first pictures of the explosive devices used in the twin blasts which hit the Boston Marathon on Monday.\n",
      "Tokenized: ['2', '05:36', ',', 'UK', ',', 'Wednesday', '17', 'April', '2013', 'US', 'authorities', 'have', 'released', 'the', 'first', 'pictures', 'of', 'the', 'explosive', 'devices', 'used', 'in', 'the', 'twin', 'blasts', 'which', 'hit', 'the', 'Boston', 'Marathon', 'on', 'Monday', '.']\n",
      "Number of tokens: 33\n",
      "\n",
      "Sentence 3:\n",
      "Original: 3\t$0.99 for first 4 weeks Subscribe Now Labor constraints and rising construction costs held back North Texas homebuilders in the most recent quarter.\n",
      "Tokenized: ['3', '$', '0.99', 'for', 'first', '4', 'weeks', 'Subscribe', 'Now', 'Labor', 'constraints', 'and', 'rising', 'construction', 'costs', 'held', 'back', 'North', 'Texas', 'homebuilders', 'in', 'the', 'most', 'recent', 'quarter', '.']\n",
      "Number of tokens: 26\n",
      "\n",
      "Sentence 4:\n",
      "Original: 4\t$0.99 for first 4 weeks Subscribe Now Updated at 10 p.m.: Revised to include the boy's name.\n",
      "Tokenized: ['4', '$', '0.99', 'for', 'first', '4', 'weeks', 'Subscribe', 'Now', 'Updated', 'at', '10', 'p.m.', ':', 'Revised', 'to', 'include', 'the', 'boy', \"'s\", 'name', '.']\n",
      "Number of tokens: 22\n",
      "\n",
      "Sentence 5:\n",
      "Original: 5\t100 YEARS AGO Jan. 29, 1910 It will be only a matter of time until the University of Arkansas will have a department of Journalism.\n",
      "Tokenized: ['5', '100', 'YEARS', 'AGO', 'Jan.', '29', ',', '1910', 'It', 'will', 'be', 'only', 'a', 'matter', 'of', 'time', 'until', 'the', 'University', 'of', 'Arkansas', 'will', 'have', 'a', 'department', 'of', 'Journalism', '.']\n",
      "Number of tokens: 28\n"
     ]
    }
   ],
   "source": [
    "# Calculate token statistics\n",
    "df['num_tokens'] = df['tokens'].apply(len)\n",
    "\n",
    "print(f\"Total tokens: {df['num_tokens'].sum():,}\")\n",
    "print(f\"Average tokens per sentence: {df['num_tokens'].mean():.2f}\")\n",
    "print(f\"Min tokens in a sentence: {df['num_tokens'].min()}\")\n",
    "print(f\"Max tokens in a sentence: {df['num_tokens'].max()}\")\n",
    "\n",
    "# Calculate token statistics\n",
    "df['num_tokens'] = df['tokens'].apply(len)\n",
    " \n",
    "print(f\"Total tokens: {df['num_tokens'].sum():,}\")\n",
    "print(f\"Average tokens per sentence: {df['num_tokens'].mean():.2f}\")\n",
    "print(f\"Min tokens in a sentence: {df['num_tokens'].min()}\")\n",
    "print(f\"Max tokens in a sentence: {df['num_tokens'].max()}\")\n",
    " \n",
    "print(f\"\\nExamples of tokenized text:\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(5):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    print(f\"Original: {df['text'].iloc[i]}\")\n",
    "    print(f\"Tokenized: {df['tokens'].iloc[i]}\")\n",
    "    print(f\"Number of tokens: {len(df['tokens'].iloc[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded08282-634c-4d4f-95d3-2afc9af1b9d0",
   "metadata": {},
   "source": [
    "## 1.2: Convert all the words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "26bed065-5517-433f-ab96-a66f477bb094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Lowercase conversion complete!\n"
     ]
    }
   ],
   "source": [
    "def lowercase_tokens(tokens):\n",
    "    \"\"\"Convert all tokens to lowercase\"\"\"\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "# Apply lowercase conversion\n",
    "df['tokens_lower'] = df['tokens'].apply(lowercase_tokens)\n",
    "\n",
    "print(\"✓ Lowercase conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d506dfc-5c3e-4d44-8842-583860dacd43",
   "metadata": {},
   "source": [
    "### Display Lowercase Conversion Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "bd3d225a-5e56-493d-ac33-04afc9e7cec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of original vs lowercase tokens:\n",
      "======================================================================\n",
      "\n",
      "Sentence 1:\n",
      "Original tokens: ['1', '!', '03alpe01', 'It', \"'s\", 'a', 'rip', 'off', '.']\n",
      "Lowercase tokens: ['1', '!', '03alpe01', 'it', \"'s\", 'a', 'rip', 'off', '.']\n",
      "\n",
      "Sentence 2:\n",
      "Original tokens: ['2', '05:36', ',', 'UK', ',', 'Wednesday', '17', 'April', '2013', 'US', 'authorities', 'have', 'released', 'the', 'first', 'pictures', 'of', 'the', 'explosive', 'devices', 'used', 'in', 'the', 'twin', 'blasts', 'which', 'hit', 'the', 'Boston', 'Marathon', 'on', 'Monday', '.']\n",
      "Lowercase tokens: ['2', '05:36', ',', 'uk', ',', 'wednesday', '17', 'april', '2013', 'us', 'authorities', 'have', 'released', 'the', 'first', 'pictures', 'of', 'the', 'explosive', 'devices', 'used', 'in', 'the', 'twin', 'blasts', 'which', 'hit', 'the', 'boston', 'marathon', 'on', 'monday', '.']\n",
      "\n",
      "Sentence 3:\n",
      "Original tokens: ['3', '$', '0.99', 'for', 'first', '4', 'weeks', 'Subscribe', 'Now', 'Labor', 'constraints', 'and', 'rising', 'construction', 'costs', 'held', 'back', 'North', 'Texas', 'homebuilders', 'in', 'the', 'most', 'recent', 'quarter', '.']\n",
      "Lowercase tokens: ['3', '$', '0.99', 'for', 'first', '4', 'weeks', 'subscribe', 'now', 'labor', 'constraints', 'and', 'rising', 'construction', 'costs', 'held', 'back', 'north', 'texas', 'homebuilders', 'in', 'the', 'most', 'recent', 'quarter', '.']\n",
      "\n",
      "Sentence 4:\n",
      "Original tokens: ['4', '$', '0.99', 'for', 'first', '4', 'weeks', 'Subscribe', 'Now', 'Updated', 'at', '10', 'p.m.', ':', 'Revised', 'to', 'include', 'the', 'boy', \"'s\", 'name', '.']\n",
      "Lowercase tokens: ['4', '$', '0.99', 'for', 'first', '4', 'weeks', 'subscribe', 'now', 'updated', 'at', '10', 'p.m.', ':', 'revised', 'to', 'include', 'the', 'boy', \"'s\", 'name', '.']\n",
      "\n",
      "Sentence 5:\n",
      "Original tokens: ['5', '100', 'YEARS', 'AGO', 'Jan.', '29', ',', '1910', 'It', 'will', 'be', 'only', 'a', 'matter', 'of', 'time', 'until', 'the', 'University', 'of', 'Arkansas', 'will', 'have', 'a', 'department', 'of', 'Journalism', '.']\n",
      "Lowercase tokens: ['5', '100', 'years', 'ago', 'jan.', '29', ',', '1910', 'it', 'will', 'be', 'only', 'a', 'matter', 'of', 'time', 'until', 'the', 'university', 'of', 'arkansas', 'will', 'have', 'a', 'department', 'of', 'journalism', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nComparison of original vs lowercase tokens:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    print(f\"Original tokens: {df['tokens'].iloc[i]}\")\n",
    "    print(f\"Lowercase tokens: {df['tokens_lower'].iloc[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f69513-164c-48f4-800b-404a7d2fc941",
   "metadata": {},
   "source": [
    "## 1.3: Handle out-of-vocabulary (OOV) words by using a special token like <UNK>.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "5e041d24-8d57-4836-bd90-769f7f2c1edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling OOV words with minimum frequency threshold = 2\n",
      "Words appearing < 2 times will be replaced with <UNK>\n"
     ]
    }
   ],
   "source": [
    "def handle_oov(tokens, word_freq, min_frequency=2):\n",
    "    \"\"\"\n",
    "    Replace rare words with <UNK> token\n",
    "    \n",
    "    Parameters:\n",
    "    - tokens: list of tokens\n",
    "    - word_freq: Counter object with word frequencies\n",
    "    - min_frequency: minimum frequency threshold\n",
    "    \n",
    "    Returns:\n",
    "    - List of tokens with rare words replaced by <UNK>\n",
    "    \"\"\"\n",
    "    return [token if word_freq[token] >= min_frequency else '<UNK>' for token in tokens]\n",
    "\n",
    "# Set minimum frequency threshold\n",
    "MIN_FREQ = 2\n",
    "\n",
    "print(f\"Handling OOV words with minimum frequency threshold = {MIN_FREQ}\")\n",
    "print(f\"Words appearing < {MIN_FREQ} times will be replaced with <UNK>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c462e-d931-4cd7-83bb-975d8c3a729d",
   "metadata": {},
   "source": [
    "### Apply OOV Handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "60660dfd-3109-4c0b-8b8c-078d5bfd8095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OOV handling complete!\n"
     ]
    }
   ],
   "source": [
    "# Build frequency counts on lowercase tokens so OOV replacement has context\n",
    "word_freq = Counter(token for tokens in df['tokens_lower'] for token in tokens)\n",
    "\n",
    "# Apply OOV handling\n",
    "df['tokens_final'] = df['tokens_lower'].apply(\n",
    "    lambda tokens: handle_oov(tokens, word_freq, MIN_FREQ)\n",
    ")\n",
    "\n",
    "print(\"✓ OOV handling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a3a23-fade-47b1-896a-3e78df9c5840",
   "metadata": {},
   "source": [
    "### Analyze Impact of OOV Handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "cbe17736-087a-40cc-a3f7-fa64807b861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Statistics After OOV Handling:\n",
      "======================================================================\n",
      "Original vocabulary size: 194,807 unique words\n",
      "Final vocabulary size: 42,219 unique words\n",
      "Vocabulary reduction: 152,588 words\n",
      "<UNK> token count: 152,589 occurrences\n",
      "Percentage of <UNK> tokens: 6.36%\n"
     ]
    }
   ],
   "source": [
    "all_tokens_final = [token for tokens in df['tokens_final'] for token in tokens]\n",
    "final_word_freq = Counter(all_tokens_final)\n",
    "\n",
    "print(f\"Vocabulary Statistics After OOV Handling:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original vocabulary size: {len(word_freq):,} unique words\")\n",
    "print(f\"Final vocabulary size: {len(final_word_freq):,} unique words\")\n",
    "print(f\"Vocabulary reduction: {len(word_freq) - len(final_word_freq):,} words\")\n",
    "print(f\"<UNK> token count: {final_word_freq.get('<UNK>', 0):,} occurrences\")\n",
    "print(f\"Percentage of <UNK> tokens: {(final_word_freq.get('<UNK>', 0) / len(all_tokens_final)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90ace3-e308-4be3-877b-5c3fbf153390",
   "metadata": {},
   "source": [
    "### Display Examples of OOV Handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "97981a69-3088-4bc1-a026-124b671ef3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of OOV handling:\n",
      "======================================================================\n",
      "\n",
      "Sentence 1:\n",
      "Before: ['1', '!', '03alpe01', 'it', \"'s\", 'a', 'rip', 'off', '.']\n",
      "After:  ['1', '!', '03alpe01', 'it', \"'s\", 'a', 'rip', 'off', '.']\n",
      "\n",
      "Sentence 2:\n",
      "Before: ['2', '05:36', ',', 'uk', ',', 'wednesday', '17', 'april', '2013', 'us', 'authorities', 'have', 'released', 'the', 'first', 'pictures', 'of', 'the', 'explosive', 'devices', 'used', 'in', 'the', 'twin', 'blasts', 'which', 'hit', 'the', 'boston', 'marathon', 'on', 'monday', '.']\n",
      "After:  ['2', '<UNK>', ',', 'uk', ',', 'wednesday', '17', 'april', '2013', 'us', 'authorities', 'have', 'released', 'the', 'first', 'pictures', 'of', 'the', 'explosive', 'devices', 'used', 'in', 'the', 'twin', 'blasts', 'which', 'hit', 'the', 'boston', 'marathon', 'on', 'monday', '.']\n",
      "        (1 word(s) replaced with <UNK>)\n",
      "\n",
      "Sentence 3:\n",
      "Before: ['3', '$', '0.99', 'for', 'first', '4', 'weeks', 'subscribe', 'now', 'labor', 'constraints', 'and', 'rising', 'construction', 'costs', 'held', 'back', 'north', 'texas', 'homebuilders', 'in', 'the', 'most', 'recent', 'quarter', '.']\n",
      "After:  ['3', '$', '0.99', 'for', 'first', '4', 'weeks', 'subscribe', 'now', 'labor', 'constraints', 'and', 'rising', 'construction', 'costs', 'held', 'back', 'north', 'texas', 'homebuilders', 'in', 'the', 'most', 'recent', 'quarter', '.']\n",
      "\n",
      "Sentence 4:\n",
      "Before: ['4', '$', '0.99', 'for', 'first', '4', 'weeks', 'subscribe', 'now', 'updated', 'at', '10', 'p.m.', ':', 'revised', 'to', 'include', 'the', 'boy', \"'s\", 'name', '.']\n",
      "After:  ['4', '$', '0.99', 'for', 'first', '4', 'weeks', 'subscribe', 'now', 'updated', 'at', '10', 'p.m.', ':', 'revised', 'to', 'include', 'the', 'boy', \"'s\", 'name', '.']\n",
      "\n",
      "Sentence 5:\n",
      "Before: ['5', '100', 'years', 'ago', 'jan.', '29', ',', '1910', 'it', 'will', 'be', 'only', 'a', 'matter', 'of', 'time', 'until', 'the', 'university', 'of', 'arkansas', 'will', 'have', 'a', 'department', 'of', 'journalism', '.']\n",
      "After:  ['5', '100', 'years', 'ago', 'jan.', '29', ',', '1910', 'it', 'will', 'be', 'only', 'a', 'matter', 'of', 'time', 'until', 'the', 'university', 'of', 'arkansas', 'will', 'have', 'a', 'department', 'of', 'journalism', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of OOV handling:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(min(5, len(df))):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    print(f\"Before: {df['tokens_lower'].iloc[i]}\")\n",
    "    print(f\"After:  {df['tokens_final'].iloc[i]}\")\n",
    "    \n",
    "    # Check if any UNK tokens were added\n",
    "    unk_count = df['tokens_final'].iloc[i].count('<UNK>')\n",
    "    if unk_count > 0:\n",
    "        print(f\"        ({unk_count} word(s) replaced with <UNK>)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5517c91f-8df6-4821-b69e-bb04f0d6618c",
   "metadata": {},
   "source": [
    "## 1.4 Preprocessing Pipeline Summary\n",
    "\n",
    "Let's create a comprehensive summary showing the transformation at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "bb0136b5-7868-4ece-b375-8e31274cdd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Pipeline Summary:\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Step</th>\n",
       "      <th>Example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original Text</td>\n",
       "      <td>1\\t! 03alpe01 It's a rip off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After Tokenization</td>\n",
       "      <td>1 ! 03alpe01 It 's a rip off .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After Lowercase</td>\n",
       "      <td>1 ! 03alpe01 it 's a rip off .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After OOV Handling</td>\n",
       "      <td>1 ! 03alpe01 it 's a rip off .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Step                         Example\n",
       "0       Original Text   1\\t! 03alpe01 It's a rip off.\n",
       "1  After Tokenization  1 ! 03alpe01 It 's a rip off .\n",
       "2     After Lowercase  1 ! 03alpe01 it 's a rip off .\n",
       "3  After OOV Handling  1 ! 03alpe01 it 's a rip off ."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create summary dataframe showing preprocessing pipeline\n",
    "summary_data = {\n",
    "    'Step': [\n",
    "        'Original Text',\n",
    "        'After Tokenization',\n",
    "        'After Lowercase',\n",
    "        'After OOV Handling'\n",
    "    ],\n",
    "    'Example': [\n",
    "        df['text'].iloc[0],\n",
    "        ' '.join(df['tokens'].iloc[0]),\n",
    "        ' '.join(df['tokens_lower'].iloc[0]),\n",
    "        ' '.join(df['tokens_final'].iloc[0])\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Preprocessing Pipeline Summary:\")\n",
    "print(\"=\" * 70)\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec0b6f-adf0-46d3-ba0d-5bfb7f63ab76",
   "metadata": {},
   "source": [
    "### *1. DATA PREPROCESSING — JUSTIFICATION*\n",
    "Tokenization\n",
    "\n",
    "Tokenizing breaks sentences into meaningful units (words, numbers, punctuation).\n",
    "This is crucial because N-gram models compute probabilities over sequences of tokens, not raw strings.\n",
    "\n",
    "Output check: punctuation like \".\", \",\", and words like \"the\", \"uk\".\n",
    "→ This confirms correct tokenization because NLTK keeps punctuation separate.\n",
    "\n",
    "Lowercasing\n",
    "\n",
    "Models treat “The” and “the” as different tokens. Lowercasing reduces sparsity, making counts more reliable.\n",
    "\n",
    "Output: \"The\" becomes \"the\" — vocabulary size reduces slightly.\n",
    "→ This matches expectation: case differences are removed.\n",
    "\n",
    "OOV Handling (<UNK>)\n",
    "\n",
    "Words that appear very rarely (frequency < MIN_FREQ) hurt probability estimation because they create zero-probability events.\n",
    "Replacing them with <UNK> groups rare words into a single token, improving robustness.\n",
    "\n",
    "Output showed:\n",
    "\n",
    "Vocabulary shrank from ~195K → ~42K\n",
    "<UNK> accounted for ~6% of tokens\n",
    "→ This is normal in real corpora because natural language has a long tail (Zipf’s law): many words occur only once.\n",
    "\n",
    "Justification:\n",
    "The preprocessing steps reduce sparsity, normalize the data, and prevent zero frequencies, making the N-gram model stable and well-behaved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de9739-2b19-4b2e-a9eb-6bb188cf29bb",
   "metadata": {},
   "source": [
    "### *Step 2. N-gram Model Construction:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a80ca9-ca38-46aa-a7f7-1ad7e948436f",
   "metadata": {},
   "source": [
    "### 2.1 Construct the N-gram model, where N is the number of words considered in a sequence (e.g., bigram for N=2, trigram for N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "a145437b-5752-4b4c-944d-e1b2097f1c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: 42219 unique\n",
      "Bigrams : 721779 unique\n",
      "Trigrams: 1604944 unique\n",
      "\n",
      "Top 10 unigrams:\n",
      "('<UNK>',) -> 152589\n",
      "('the',) -> 115432\n",
      "(',',) -> 100578\n",
      "('.',) -> 97988\n",
      "('to',) -> 56261\n",
      "('and',) -> 53029\n",
      "('of',) -> 48958\n",
      "('a',) -> 45222\n",
      "('in',) -> 39157\n",
      "('for',) -> 21400\n",
      "\n",
      "Top 10 bigrams:\n",
      "<s> <UNK> -> 98566\n",
      ". </s> -> 89070\n",
      "<UNK> the -> 11804\n",
      "of the -> 11794\n",
      "’ s -> 10903\n",
      "in the -> 10024\n",
      ", and -> 6846\n",
      "<UNK> “ -> 6379\n",
      "<UNK> <UNK> -> 5868\n",
      "<UNK> , -> 5819\n",
      "\n",
      "Top 10 trigrams:\n",
      "<s> <s> <UNK> -> 98566\n",
      "<s> <UNK> the -> 11220\n",
      "<s> <UNK> “ -> 6314\n",
      "<UNK> . </s> -> 3807\n",
      ". ” </s> -> 3612\n",
      "<s> <UNK> '' -> 3465\n",
      "<s> <UNK> <UNK> -> 3421\n",
      "said . </s> -> 3304\n",
      "<s> <UNK> it -> 3069\n",
      "<s> <UNK> i -> 2874\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "\n",
    "def add_sentence_boundaries(tokens, n):\n",
    "    \"\"\"\n",
    "    Add sentence boundary tokens for an N-gram model.\n",
    "\n",
    "    We add (n - 1) <s> tokens at the start and 1 </s> at the end.\n",
    "    This helps the model learn how sentences begin and end.\n",
    "    \"\"\"\n",
    "    if n <= 1:\n",
    "        # For unigram, we usually do not need multiple start tokens\n",
    "        return tokens + [END_TOKEN]\n",
    "    num_start_tokens = n - 1\n",
    "    return [START_TOKEN] * num_start_tokens + tokens + [END_TOKEN]\n",
    "\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    Generate all N-grams from a list of tokens.\n",
    "\n",
    "    Each N-gram is returned as a tuple of length n.\n",
    "    Example:\n",
    "        tokens = ['<s>', 'i', 'love', 'nlp', '</s>'], n = 2\n",
    "        -> [('<s>', 'i'), ('i', 'love'), ('love', 'nlp'), ('nlp', '</s>')]\n",
    "    \"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "\n",
    "def build_ngram_model(df, token_column, n, use_boundaries=True):\n",
    "    \"\"\"\n",
    "    Build an N-gram model (frequency counts) from a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df            : pandas DataFrame with a column of token lists\n",
    "    - token_column  : column name that contains tokenized sentences\n",
    "                      (in your case: 'tokens_final')\n",
    "    - n             : size of the N-gram (e.g., 1, 2, 3, ...)\n",
    "    - use_boundaries: if True, add <s> and </s> based on n\n",
    "\n",
    "    Returns:\n",
    "    - Counter mapping: N-gram tuple -> frequency\n",
    "    \"\"\"\n",
    "    ngram_counts = Counter()\n",
    "\n",
    "    for tokens in df[token_column]:\n",
    "        if use_boundaries:\n",
    "            tokens_used = add_sentence_boundaries(tokens, n)\n",
    "        else:\n",
    "            tokens_used = tokens\n",
    "\n",
    "        ngrams = generate_ngrams(tokens_used, n)\n",
    "        ngram_counts.update(ngrams)\n",
    "\n",
    "    return ngram_counts\n",
    "\n",
    "# Build unigram, bigram, trigram models using the preprocessed tokens_final\n",
    "unigram_counts = build_ngram_model(df, token_column=\"tokens_final\", n=1, use_boundaries=False)\n",
    "bigram_counts  = build_ngram_model(df, token_column=\"tokens_final\", n=2, use_boundaries=True)\n",
    "trigram_counts = build_ngram_model(df, token_column=\"tokens_final\", n=3, use_boundaries=True)\n",
    "\n",
    "print(\"Unigrams:\", len(unigram_counts), \"unique\")\n",
    "print(\"Bigrams :\", len(bigram_counts), \"unique\")\n",
    "print(\"Trigrams:\", len(trigram_counts), \"unique\")\n",
    "\n",
    "# Look at a few examples\n",
    "print(\"\\nTop 10 unigrams:\")\n",
    "for ng, c in unigram_counts.most_common(10):\n",
    "    print(ng, \"->\", c)\n",
    "\n",
    "print(\"\\nTop 10 bigrams:\")\n",
    "for ng, c in bigram_counts.most_common(10):\n",
    "    print(\" \".join(ng), \"->\", c)\n",
    "\n",
    "print(\"\\nTop 10 trigrams:\")\n",
    "for ng, c in trigram_counts.most_common(10):\n",
    "    print(\" \".join(ng), \"->\", c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25693cce-ad35-4526-9ac5-cc53f2486a09",
   "metadata": {},
   "source": [
    "### 2.2 Count the frequency of each N-gram (a sequence of N consecutive words) in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "e9d4ad33-52f5-484d-9021-224773ec1490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting N-gram frequencies in the training corpus...\n",
      "\n",
      "Unigram Statistics\n",
      "------------------------------------------------------------\n",
      "Total unigrams       : 2,400,354\n",
      "Unique unigrams      : 42,219\n",
      "Top 10 most frequent unigrams:\n",
      "\n",
      "<UNK>                          -> 152,589\n",
      "the                            -> 115,432\n",
      ",                              -> 100,578\n",
      ".                              -> 97,988\n",
      "to                             -> 56,261\n",
      "and                            -> 53,029\n",
      "of                             -> 48,958\n",
      "a                              -> 45,222\n",
      "in                             -> 39,157\n",
      "for                            -> 21,400\n",
      "\n",
      "\n",
      "Bigram Statistics\n",
      "------------------------------------------------------------\n",
      "Total bigrams       : 2,500,354\n",
      "Unique bigrams      : 721,779\n",
      "Top 10 most frequent bigrams:\n",
      "\n",
      "<s> <UNK>                      -> 98,566\n",
      ". </s>                         -> 89,070\n",
      "<UNK> the                      -> 11,804\n",
      "of the                         -> 11,794\n",
      "’ s                            -> 10,903\n",
      "in the                         -> 10,024\n",
      ", and                          -> 6,846\n",
      "<UNK> “                        -> 6,379\n",
      "<UNK> <UNK>                    -> 5,868\n",
      "<UNK> ,                        -> 5,819\n",
      "\n",
      "\n",
      "Trigram Statistics\n",
      "------------------------------------------------------------\n",
      "Total trigrams       : 2,500,354\n",
      "Unique trigrams      : 1,604,944\n",
      "Top 10 most frequent trigrams:\n",
      "\n",
      "<s> <s> <UNK>                  -> 98,566\n",
      "<s> <UNK> the                  -> 11,220\n",
      "<s> <UNK> “                    -> 6,314\n",
      "<UNK> . </s>                   -> 3,807\n",
      ". ” </s>                       -> 3,612\n",
      "<s> <UNK> ''                   -> 3,465\n",
      "<s> <UNK> <UNK>                -> 3,421\n",
      "said . </s>                    -> 3,304\n",
      "<s> <UNK> it                   -> 3,069\n",
      "<s> <UNK> i                    -> 2,874\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Count the frequency of each N-gram in the training corpus\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "print(\"Counting N-gram frequencies in the training corpus...\\n\")\n",
    "\n",
    "# We already created the unigram, bigram, trigram counts above\n",
    "# Below, we simply display the required frequency results clearly.\n",
    "\n",
    "def display_ngram_stats(name, counter, top_k=10):\n",
    "    \"\"\"\n",
    "    Helper to display N-gram frequency statistics.\n",
    "    \"\"\"\n",
    "    print(f\"{name} Statistics\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total {name.lower()}s       : {sum(counter.values()):,}\")\n",
    "    print(f\"Unique {name.lower()}s      : {len(counter):,}\")\n",
    "    print(f\"Top {top_k} most frequent {name.lower()}s:\")\n",
    "    print()\n",
    "\n",
    "    for ngram, count in counter.most_common(top_k):\n",
    "        # Convert ('the',) to \"the\"\n",
    "        if len(ngram) == 1:\n",
    "            ngram_str = ngram[0]\n",
    "        else:\n",
    "            ngram_str = \" \".join(ngram)\n",
    "        print(f\"{ngram_str:30s} -> {count:,}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# Display frequency info for each model\n",
    "display_ngram_stats(\"Unigram\", unigram_counts)\n",
    "display_ngram_stats(\"Bigram\", bigram_counts)\n",
    "display_ngram_stats(\"Trigram\", trigram_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4883eaa0-d674-4c49-9f2a-03debcaed1e6",
   "metadata": {},
   "source": [
    "### ***2. N-GRAM MODEL CONSTRUCTION — JUSTIFICATION***\n",
    "\n",
    "The model must learn where sentences begin and end.\n",
    "\n",
    "Adding (n–1) tokens lets an N-gram model predict the first few words.\n",
    "Example: In a bigram model, the teaches how sentences start.\n",
    "\n",
    "Frequency counts\n",
    "\n",
    "N-grams represent contiguous word sequences.\n",
    "\n",
    "Frequency of N-grams is essential input for probability estimation.\n",
    "\n",
    "Expected patterns (observed in output):\n",
    "\n",
    "High-frequency unigrams: the, ,, ., and, of\n",
    "→ These are always top words in English corpora.\n",
    "\n",
    "High-frequency bigrams: of the, in the, to the\n",
    "→ These are common English collocations.\n",
    "\n",
    "Rare N-grams: extremely many\n",
    "→ Language is sparse; higher-order N-grams are exponentially rarer.\n",
    "\n",
    "Justification:\n",
    "The N-gram counts match known English distribution statistics (Zipf’s law), indicating the model is correctly counting sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e868f-6f60-4683-8498-8daa16f0b20d",
   "metadata": {},
   "source": [
    "### Step 3. Probability Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81557489-6e54-4953-8a6e-621e890c584c",
   "metadata": {},
   "source": [
    "### 3.1: Estimate the probability of a word sequence by computing the conditional probability of each word given its preceding N-1 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "651bcef0-c7bb-4977-a2af-a2c56c51f53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability Estimation\n",
      "\n",
      "[MLE] N = 1\n",
      "Total log P(sentence): -inf\n",
      "Avg log P per word   : -inf\n",
      "\n",
      "[MLE] N = 2\n",
      "Total log P(sentence): -inf\n",
      "Avg log P per word   : -inf\n",
      "\n",
      "[MLE] N = 3\n",
      "Total log P(sentence): -inf\n",
      "Avg log P per word   : -inf\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3.1 Probability Estimation (NO SMOOTHING, pure MLE)\n",
    "#     P(w_t | history) = count(history, w_t) / count(history)\n",
    "#     - For unigrams: P(w) = count(w) / total_tokens\n",
    "#     - Unseen n-grams → probability = 0\n",
    "# ============================================================\n",
    "\n",
    "# Total number of tokens for unigram MLE\n",
    "TOTAL_UNIGRAM_TOKENS = sum(unigram_counts.values())\n",
    "\n",
    "\n",
    "def ngram_probability_mle(ngram, ngram_counts, lower_order_counts=None):\n",
    "    \"\"\"\n",
    "    Compute the MLE probability of an N-gram (no smoothing).\n",
    "\n",
    "    - For unigrams: P(w) = count(w) / TOTAL_UNIGRAM_TOKENS\n",
    "    - For N > 1:   P(w_t | history) = count(history, w_t) / count(history)\n",
    "\n",
    "    Returns 0.0 if:\n",
    "    - the ngram is unseen, or\n",
    "    - the history never occurs.\n",
    "    \"\"\"\n",
    "    n = len(ngram)\n",
    "\n",
    "    # Unigram: P(w)\n",
    "    if n == 1:\n",
    "        count_w = ngram_counts.get(ngram, 0)\n",
    "        if TOTAL_UNIGRAM_TOKENS == 0:\n",
    "            return 0.0\n",
    "        return count_w / TOTAL_UNIGRAM_TOKENS\n",
    "\n",
    "    # N > 1: P(w_t | history)\n",
    "    history = ngram[:-1]\n",
    "    ngram_count = ngram_counts.get(ngram, 0)\n",
    "    history_count = lower_order_counts.get(history, 0) if lower_order_counts is not None else 0\n",
    "\n",
    "    if history_count == 0 or ngram_count == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return ngram_count / history_count\n",
    "\n",
    "\n",
    "def sentence_log_probability_mle(tokens, n):\n",
    "    \"\"\"\n",
    "    Compute log P(sentence) using an N-gram MLE model (no smoothing).\n",
    "\n",
    "    - tokens: preprocessed + OOV-handled sentence (e.g., from df[\"tokens_final\"])\n",
    "    - n     : order of the model (1, 2, or 3)\n",
    "\n",
    "    Returns:\n",
    "    - total_log_prob : log P(sentence)\n",
    "    - avg_log_prob   : average log prob per word\n",
    "\n",
    "    If any n-gram has probability 0 → returns -inf.\n",
    "    \"\"\"\n",
    "    # Match boundary handling with model training\n",
    "    tokens_padded = add_sentence_boundaries(tokens, n)\n",
    "    ngrams = generate_ngrams(tokens_padded, n)\n",
    "\n",
    "    total_log_prob = 0.0\n",
    "\n",
    "    for ng in ngrams:\n",
    "        if n == 1:\n",
    "            p = ngram_probability_mle(\n",
    "                ng,\n",
    "                ngram_counts=unigram_counts,\n",
    "                lower_order_counts=None,\n",
    "            )\n",
    "        elif n == 2:\n",
    "            p = ngram_probability_mle(\n",
    "                ng,\n",
    "                ngram_counts=bigram_counts,\n",
    "                lower_order_counts=unigram_counts,\n",
    "            )\n",
    "        elif n == 3:\n",
    "            p = ngram_probability_mle(\n",
    "                ng,\n",
    "                ngram_counts=trigram_counts,\n",
    "                lower_order_counts=bigram_counts,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"MLE helper currently supports N = 1, 2, 3 only.\")\n",
    "\n",
    "        if p <= 0.0:\n",
    "            # Unseen n-gram → log(0) = -inf\n",
    "            return -np.inf, -np.inf\n",
    "\n",
    "        total_log_prob += np.log(p)\n",
    "\n",
    "    avg_log_prob = total_log_prob / max(len(tokens), 1)\n",
    "    return total_log_prob, avg_log_prob\n",
    "\n",
    "\n",
    "example_tokens = df[\"tokens_final\"].iloc[0]\n",
    "print(\"Probability Estimation\")\n",
    "for n in [1, 2, 3]:\n",
    "    total_log_p, avg_log_p = sentence_log_probability_mle(example_tokens, n)\n",
    "    print(f\"\\n[MLE] N = {n}\")\n",
    "    print(f\"Total log P(sentence): {total_log_p}\")\n",
    "    print(f\"Avg log P per word   : {avg_log_p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1d005-bac1-461a-9a3a-a1e1f1f04875",
   "metadata": {},
   "source": [
    "### 3.2 Use smoothing techniques (such as Laplace smoothing) to handle unseen N-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "6228ae55-4430-4afb-88f1-9c5cc1c0d682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Laplace] N = 1, α = 1.0\n",
      "Total log P(sentence): -81.6864\n",
      "Avg log P per word   : -9.0763\n",
      "\n",
      "[Laplace] N = 2, α = 1.0\n",
      "Total log P(sentence): -75.1647\n",
      "Avg log P per word   : -8.3516\n",
      "\n",
      "[Laplace] N = 3, α = 1.0\n",
      "Total log P(sentence): -90.9731\n",
      "Avg log P per word   : -10.1081\n",
      "\n",
      "P_Laplace('the' | '<s>') = 0.000024\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3.2 Probability Estimation WITH Laplace Smoothing\n",
    "#     P_Laplace(w_t | history) =\n",
    "#        (count(history, w_t) + α) / (count(history) + α * V)\n",
    "#\n",
    "#     - Handles unseen N-grams by giving them small non-zero prob\n",
    "# ============================================================\n",
    "\n",
    "# Vocabulary size (based on unigrams)\n",
    "V = len(unigram_counts)\n",
    "TOTAL_UNIGRAM_TOKENS = sum(unigram_counts.values())\n",
    "\n",
    "\n",
    "def ngram_probability_laplace(ngram, ngram_counts, lower_order_counts=None, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Compute P(w_t | history) for an N-gram using Laplace smoothing.\n",
    "\n",
    "    - Unigram:\n",
    "        P(w) = (count(w) + α) / (TOTAL_UNIGRAM_TOKENS + α * V)\n",
    "\n",
    "    - N > 1:\n",
    "        P(w_t | history) = (count(history, w_t) + α) /\n",
    "                           (count(history) + α * V)\n",
    "\n",
    "    If history never appears (history_count = 0), we still get a valid\n",
    "    probability because denominator = α * V.\n",
    "    \"\"\"\n",
    "    n = len(ngram)\n",
    "\n",
    "    # Unigram: P(w)\n",
    "    if n == 1:\n",
    "        count_w = ngram_counts.get(ngram, 0)\n",
    "        numerator = count_w + alpha\n",
    "        denominator = TOTAL_UNIGRAM_TOKENS + alpha * V\n",
    "        return numerator / denominator\n",
    "\n",
    "    # N > 1: P(w_t | history)\n",
    "    history = ngram[:-1]\n",
    "    ngram_count = ngram_counts.get(ngram, 0)\n",
    "    history_count = lower_order_counts.get(history, 0) if lower_order_counts is not None else 0\n",
    "\n",
    "    numerator = ngram_count + alpha\n",
    "    denominator = history_count + alpha * V\n",
    "\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def sentence_log_probability_laplace(tokens, n, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Compute log P(sentence) using an N-gram model\n",
    "    with Laplace smoothing.\n",
    "\n",
    "    - tokens: preprocessed + OOV-handled sentence\n",
    "    - n     : order of the model (1, 2, or 3)\n",
    "    - alpha : smoothing parameter (α = 1.0 by default)\n",
    "\n",
    "    Returns:\n",
    "    - total_log_prob : log P(sentence)\n",
    "    - avg_log_prob   : average log prob per word\n",
    "    \"\"\"\n",
    "    tokens_padded = add_sentence_boundaries(tokens, n)\n",
    "    ngrams = generate_ngrams(tokens_padded, n)\n",
    "\n",
    "    total_log_prob = 0.0\n",
    "\n",
    "    for ng in ngrams:\n",
    "        if n == 1:\n",
    "            p = ngram_probability_laplace(\n",
    "                ng,\n",
    "                ngram_counts=unigram_counts,\n",
    "                lower_order_counts=None,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "        elif n == 2:\n",
    "            p = ngram_probability_laplace(\n",
    "                ng,\n",
    "                ngram_counts=bigram_counts,\n",
    "                lower_order_counts=unigram_counts,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "        elif n == 3:\n",
    "            p = ngram_probability_laplace(\n",
    "                ng,\n",
    "                ngram_counts=trigram_counts,\n",
    "                lower_order_counts=bigram_counts,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"Laplace helper currently supports N = 1, 2, 3 only.\")\n",
    "\n",
    "        total_log_prob += np.log(p)\n",
    "\n",
    "    avg_log_prob = total_log_prob / max(len(tokens), 1)\n",
    "    return total_log_prob, avg_log_prob\n",
    "\n",
    "\n",
    "# --- Example usage (with Laplace smoothing) ---\n",
    "example_tokens = df[\"tokens_final\"].iloc[0]\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    total_log_p, avg_log_p = sentence_log_probability_laplace(example_tokens, n, alpha=1.0)\n",
    "    print(f\"\\n[Laplace] N = {n}, α = 1.0\")\n",
    "    print(f\"Total log P(sentence): {total_log_p:.4f}\")\n",
    "    print(f\"Avg log P per word   : {avg_log_p:.4f}\")\n",
    "\n",
    "# Example: single conditional probability with smoothing\n",
    "history = (\"<s>\",)\n",
    "word = \"the\"\n",
    "bigram = history + (word,)\n",
    "\n",
    "p_bigram = ngram_probability_laplace(\n",
    "    bigram,\n",
    "    ngram_counts=bigram_counts,\n",
    "    lower_order_counts=unigram_counts,\n",
    "    alpha=1.0,\n",
    ")\n",
    "print(f\"\\nP_Laplace('{word}' | '<s>') = {p_bigram:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c779d-f8d1-4fa1-85d1-85f2fd2be23b",
   "metadata": {},
   "source": [
    "### ***3. PROBABILITY ESTIMATION — JUSTIFICATION***\n",
    "3.1 MLE (Maximum Likelihood Estimation)\n",
    "\n",
    "Output showed:\n",
    "\n",
    "MLE produced -inf log probability\n",
    "→ Expected and correct.\n",
    "→ Because if any N-gram in the test sentence never occurred in training, its probability becomes 0, and:\n",
    "\n",
    "\n",
    "Why this is correct:\n",
    "MLE gives zero probability to unseen N-grams — common in real data, especially for bigrams/trigrams.\n",
    "\n",
    "3.2 Laplace / Lidstone (Add-k) Smoothing — Justification\n",
    "\n",
    "Output with α = 1.0 or 0.1 gave:\n",
    "\n",
    "finite log probabilities\n",
    "\n",
    "lower perplexities\n",
    "\n",
    "This is correct because smoothing eliminates zero probabilities by adding a small constant:\n",
    "\n",
    "\n",
    "\n",
    "This guarantees:\n",
    "\n",
    "No zero probabilities\n",
    "\n",
    "Better generalization\n",
    "\n",
    "More stable evaluation on unseen data\n",
    "\n",
    "Smoothing reduces model brittleness — exactly what your results show."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f2a2d-f6ce-441a-beb1-240e45f13f17",
   "metadata": {},
   "source": [
    "### *Step 4.Text Generation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ed277-2030-416f-843c-05a92ca677e0",
   "metadata": {},
   "source": [
    "### 4.1 Generate new text by predicting the next word in a sequence. Start with an initial seed phrase, and use the N-gram model to generate the following words, updating the sequence at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "70ea567a-e4e8-466c-adeb-ca717abf2ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size for generation: 42219 words\n",
      "Seed sentence: The president of the united\n",
      "\n",
      "Generated with 1-gram model (Laplace smoothing):\n",
      "the president of the united march under the a defendant tackle said south <UNK> have progress few long skills in nominate conspiracy despite a “\n",
      "\n",
      "Generated with 2-gram model (Laplace smoothing):\n",
      "the president of the united march under the a defendant tackle said south <UNK> have progress few long skills in nominate conspiracy despite a “\n",
      "\n",
      "Generated with 2-gram model (Laplace smoothing):\n",
      "the president of the united mr. capitalize bothered coin proceedings jakob lawsuits planet pursuing reclaiming adoring skydome brom nominee co-captain sigourney respiratory brouk livelihoods proposition\n",
      "\n",
      "Generated with 3-gram model (Laplace smoothing):\n",
      "the president of the united mr. capitalize bothered coin proceedings jakob lawsuits planet pursuing reclaiming adoring skydome brom nominee co-captain sigourney respiratory brouk livelihoods proposition\n",
      "\n",
      "Generated with 3-gram model (Laplace smoothing):\n",
      "the president of the united brevard bergh fishing transports 9:15 facie seeley eritreans 66066 ninh vipers mayville sha 1460 outdoor billionaire plodding manpower clerks medicine\n",
      "the president of the united brevard bergh fishing transports 9:15 facie seeley eritreans 66066 ninh vipers mayville sha 1460 outdoor billionaire plodding manpower clerks medicine\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 4. TEXT GENERATION WITH N-GRAM MODEL\n",
    "#    Generate text by predicting the next word in a sequence\n",
    "# ==========================================================\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Build a clean 1D vocabulary (list of word strings)\n",
    "# N-gram keys are tuples like ('the',), so extract the word\n",
    "# ----------------------------------------------------------\n",
    "WORD_VOCAB = sorted({ w for (w,) in unigram_counts.keys() })\n",
    "\n",
    "print(f\"Vocabulary size for generation: {len(WORD_VOCAB)} words\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Seed preprocessing: tokenize, lowercase, OOV handling\n",
    "# ----------------------------------------------------------\n",
    "def preprocess_seed(sentence, word_freq, min_frequency=2):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = handle_oov(tokens, word_freq, min_frequency)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Helper to get the correct counters for N=1,2,3\n",
    "# ----------------------------------------------------------\n",
    "def get_ngram_counts_for_order(n):\n",
    "    if n == 1:\n",
    "        return unigram_counts, None\n",
    "    elif n == 2:\n",
    "        return bigram_counts, unigram_counts\n",
    "    elif n == 3:\n",
    "        return trigram_counts, bigram_counts\n",
    "    else:\n",
    "        raise NotImplementedError(\"Generation supports only n=1,2,3.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Sample next word using the N-gram probability distribution\n",
    "# ----------------------------------------------------------\n",
    "def sample_next_word(history_tokens, n, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Sample the next word given history using Laplace smoothed probabilities.\n",
    "    \"\"\"\n",
    "    vocab = WORD_VOCAB     # 1-D list of strings\n",
    "\n",
    "    # Determine appropriate history length\n",
    "    if n == 1:\n",
    "        history = ()\n",
    "    else:\n",
    "        needed = n - 1\n",
    "        history = tuple(history_tokens[-needed:]) if len(history_tokens) >= needed else tuple(history_tokens)\n",
    "\n",
    "    ngram_counts, lower_order_counts = get_ngram_counts_for_order(n)\n",
    "\n",
    "    # Compute probability for each word in vocabulary\n",
    "    probs = []\n",
    "    for w in vocab:\n",
    "        ngram = history + (w,)\n",
    "        p = ngram_probability_laplace(\n",
    "            ngram,\n",
    "            ngram_counts=ngram_counts,\n",
    "            lower_order_counts=lower_order_counts,\n",
    "            alpha=alpha,     # Laplace smoothing\n",
    "        )\n",
    "        probs.append(p)\n",
    "\n",
    "    probs = np.array(probs, dtype=float)\n",
    "    probs = probs / probs.sum()     # normalize to sum=1\n",
    "\n",
    "    # Now safe: vocab is 1D list of strings + 1D prob array\n",
    "    next_word = np.random.choice(vocab, p=probs)\n",
    "    return next_word\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# MAIN TEXT GENERATION FUNCTION\n",
    "# ----------------------------------------------------------\n",
    "def generate_text(seed_sentence, n=3, num_words=20, alpha=1.0, min_frequency=2):\n",
    "    \"\"\"\n",
    "    Generate text using an N-gram language model with smoothing.\n",
    "    \"\"\"\n",
    "    # 1. Preprocess seed\n",
    "    seed_tokens = preprocess_seed(seed_sentence, word_freq, min_frequency=min_frequency)\n",
    "\n",
    "    # 2. History initialization\n",
    "    history = seed_tokens.copy()\n",
    "\n",
    "    if n > 1:\n",
    "        needed = n - 1\n",
    "        if len(history) < needed:\n",
    "            history = [START_TOKEN] * (needed - len(history)) + history\n",
    "\n",
    "    generated = seed_tokens.copy()\n",
    "\n",
    "    # 3. Iterative generation\n",
    "    for _ in range(num_words):\n",
    "        next_word = sample_next_word(history, n=n, alpha=alpha)\n",
    "\n",
    "        if next_word == END_TOKEN:\n",
    "            break\n",
    "\n",
    "        generated.append(next_word)\n",
    "        history.append(next_word)\n",
    "\n",
    "    # 4. Simple detokenization\n",
    "    return \" \".join(generated)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# DEMO: GENERATE TEXT FROM A SEED SENTENCE\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "seed = \"The president of the united\"\n",
    "print(\"Seed sentence:\", seed)\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    print(f\"\\nGenerated with {n}-gram model (Laplace smoothing):\")\n",
    "    gen = generate_text(seed_sentence=seed, n=n, num_words=20, alpha=1.0, min_frequency=MIN_FREQ)\n",
    "    print(gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a249226-7150-4eb1-b112-61bf90e081f5",
   "metadata": {},
   "source": [
    "### *4. TEXT GENERATION — JUSTIFICATION*\n",
    "\n",
    "Your text generation uses:\n",
    "\n",
    "The previous (N-1) tokens as history\n",
    "\n",
    "Sampling next word using smoothed probability distribution\n",
    "\n",
    "Stopping when </s> is generated or reaching max length\n",
    "\n",
    "Expected behavior (matches your output):\n",
    "\n",
    "Unigram model produces random-looking text (no context)\n",
    "\n",
    "Bigram model produces reasonable phrase-level fluency\n",
    "\n",
    "Trigram model produces more coherent, longer, realistic structure\n",
    "→ linguistically correct: higher N captures more context\n",
    "\n",
    "Example justification:\n",
    "\n",
    "The trigram model generates more fluent text because it conditions on the previous two words, capturing short phrases and local grammar patterns (e.g., “in the”, “as part of”, “on Monday”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b65ed-1509-47e7-8093-4eb45a213e09",
   "metadata": {},
   "source": [
    "### Step 5. Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f580d905-6704-4ea2-a268-ec0d6f69caf4",
   "metadata": {},
   "source": [
    "### 5.1 Split the corpus into a training set and a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d519756",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "##### Objective\n",
    "- Split corpus into training and test sets\n",
    "- Evaluate model performance using perplexity\n",
    "- Compare different N-gram sizes and smoothing techniques\n",
    "\n",
    "##### Justification\n",
    "**Perplexity** measures how well the model predicts the test set:\n",
    "- Lower perplexity = better model\n",
    "- Formula: `Perplexity = 2^(-log2(P(test_set)) / N)`\n",
    "- Indicates average branching factor (how many words the model considers equally likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "72cdab49-d69d-4ac9-8039-07cededc98b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Test Split\n",
      "------------------------------------------------------------\n",
      "Total sentences : 100,000\n",
      "Train sentences : 80,000 (80%)\n",
      "Test sentences  : 20,000  (20%)\n",
      "\n",
      "Training N-gram Models\n",
      "------------------------------------------------------------\n",
      "Vocabulary size (train)     : 41,437\n",
      "Total unigram tokens (train): 1,918,710\n",
      "Unique unigrams             : 41,437\n",
      "Unique bigrams              : 612,067\n",
      "Unique trigrams             : 1,315,466\n",
      "\n",
      "Training N-gram Models\n",
      "------------------------------------------------------------\n",
      "Vocabulary size (train)     : 41,437\n",
      "Total unigram tokens (train): 1,918,710\n",
      "Unique unigrams             : 41,437\n",
      "Unique bigrams              : 612,067\n",
      "Unique trigrams             : 1,315,466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 5. MODEL EVALUATION\n",
    "# 5.1 Split corpus into train and test\n",
    "# 5.2 Evaluate N-gram models on test set (log-likelihood, perplexity)\n",
    "# ==========================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5.1 TRAIN–TEST SPLIT (sentence level)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# We randomly shuffle sentence indices and use 80% for training, 20% for testing\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(df))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split_idx = int(0.8 * len(df))\n",
    "train_idx = indices[:split_idx]\n",
    "test_idx  = indices[split_idx:]\n",
    "\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "test_df  = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Train / Test Split\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total sentences : {len(df):,}\")\n",
    "print(f\"Train sentences : {len(train_df):,} (80%)\")\n",
    "print(f\"Test sentences  : {len(test_df):,}  (20%)\\n\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Rebuild N-gram models using ONLY the training set\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "train_unigram_counts = build_ngram_model(\n",
    "    train_df, token_column=\"tokens_final\", n=1, use_boundaries=False\n",
    ")\n",
    "train_bigram_counts  = build_ngram_model(\n",
    "    train_df, token_column=\"tokens_final\", n=2, use_boundaries=True\n",
    ")\n",
    "train_trigram_counts = build_ngram_model(\n",
    "    train_df, token_column=\"tokens_final\", n=3, use_boundaries=True\n",
    ")\n",
    "\n",
    "# Vocabulary size and total tokens based on training set only\n",
    "V_train = len(train_unigram_counts)\n",
    "TOTAL_UNIGRAM_TOKENS_TRAIN = sum(train_unigram_counts.values())\n",
    "\n",
    "print(\"Training N-gram Models\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Vocabulary size (train)     : {V_train:,}\")\n",
    "print(f\"Total unigram tokens (train): {TOTAL_UNIGRAM_TOKENS_TRAIN:,}\")\n",
    "print(f\"Unique unigrams             : {len(train_unigram_counts):,}\")\n",
    "print(f\"Unique bigrams              : {len(train_bigram_counts):,}\")\n",
    "print(f\"Unique trigrams             : {len(train_trigram_counts):,}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5796f44-22bf-4078-8669-19638cc3c301",
   "metadata": {},
   "source": [
    "### 5.2 Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "1fc34708-b821-470e-9a96-471d9a77cb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluation] N = 1 (Laplace smoothing, alpha=1.0)\n",
      "------------------------------------------------------------\n",
      "Total tokens in test set        : 481,644\n",
      "Average log P per token (test)  : -7.4898\n",
      "Perplexity on test set          : 1789.7763\n",
      "\n",
      "[Evaluation] N = 2 (Laplace smoothing, alpha=1.0)\n",
      "------------------------------------------------------------\n",
      "Total tokens in test set        : 481,644\n",
      "Average log P per token (test)  : -7.8186\n",
      "Perplexity on test set          : 2486.4977\n",
      "\n",
      "[Evaluation] N = 2 (Laplace smoothing, alpha=1.0)\n",
      "------------------------------------------------------------\n",
      "Total tokens in test set        : 481,644\n",
      "Average log P per token (test)  : -7.8186\n",
      "Perplexity on test set          : 2486.4977\n",
      "\n",
      "[Evaluation] N = 3 (Laplace smoothing, alpha=1.0)\n",
      "------------------------------------------------------------\n",
      "Total tokens in test set        : 481,644\n",
      "Average log P per token (test)  : -9.6235\n",
      "Perplexity on test set          : 15115.8736\n",
      "\n",
      "[Evaluation] N = 3 (Laplace smoothing, alpha=1.0)\n",
      "------------------------------------------------------------\n",
      "Total tokens in test set        : 481,644\n",
      "Average log P per token (test)  : -9.6235\n",
      "Perplexity on test set          : 15115.8736\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Avg log P per token</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-7.489846</td>\n",
       "      <td>1789.776264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-7.818630</td>\n",
       "      <td>2486.497699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-9.623501</td>\n",
       "      <td>15115.873644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N  Avg log P per token    Perplexity\n",
       "0  1            -7.489846   1789.776264\n",
       "1  2            -7.818630   2486.497699\n",
       "2  3            -9.623501  15115.873644"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 5.2 PROBABILITY + PERPLEXITY ON TEST SET (with smoothing)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_ngram_probability_with_model(\n",
    "    ngram,\n",
    "    ngram_counts,\n",
    "    lower_order_counts,\n",
    "    V,\n",
    "    total_unigram_tokens,\n",
    "    alpha=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Laplace-smoothed probability for a given N-gram under a specific model\n",
    "    (train-only counts).\n",
    "    \"\"\"\n",
    "    n = len(ngram)\n",
    "\n",
    "    # Unigram case: P(w) = (count(w) + α) / (total_tokens + α * V)\n",
    "    if n == 1:\n",
    "        count_w = ngram_counts.get(ngram, 0)\n",
    "        numerator   = count_w + alpha\n",
    "        denominator = total_unigram_tokens + alpha * V\n",
    "        return numerator / denominator\n",
    "\n",
    "    # N > 1: P(w_t | history) = (count(history, w_t) + α) / (count(history) + α * V)\n",
    "    history = ngram[:-1]\n",
    "    ngram_count   = ngram_counts.get(ngram, 0)\n",
    "    history_count = lower_order_counts.get(history, 0) if lower_order_counts is not None else 0\n",
    "\n",
    "    numerator   = ngram_count + alpha\n",
    "    denominator = history_count + alpha * V\n",
    "\n",
    "    if denominator == 0:\n",
    "        # fallback (completely unseen history)\n",
    "        return 1.0 / V\n",
    "\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def sentence_log_probability_with_model(\n",
    "    tokens,\n",
    "    n,\n",
    "    train_unigram_counts,\n",
    "    train_bigram_counts,\n",
    "    train_trigram_counts,\n",
    "    V,\n",
    "    total_unigram_tokens,\n",
    "    alpha=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute log P(sentence) using an N-gram model trained on the training set,\n",
    "    with Laplace smoothing.\n",
    "    \"\"\"\n",
    "    tokens_padded = add_sentence_boundaries(tokens, n)\n",
    "    ngrams = generate_ngrams(tokens_padded, n)\n",
    "\n",
    "    total_log_prob = 0.0\n",
    "\n",
    "    for ng in ngrams:\n",
    "        if n == 1:\n",
    "            prob = compute_ngram_probability_with_model(\n",
    "                ng,\n",
    "                ngram_counts=train_unigram_counts,\n",
    "                lower_order_counts=None,\n",
    "                V=V,\n",
    "                total_unigram_tokens=total_unigram_tokens,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "        elif n == 2:\n",
    "            prob = compute_ngram_probability_with_model(\n",
    "                ng,\n",
    "                ngram_counts=train_bigram_counts,\n",
    "                lower_order_counts=train_unigram_counts,\n",
    "                V=V,\n",
    "                total_unigram_tokens=total_unigram_tokens,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "        elif n == 3:\n",
    "            prob = compute_ngram_probability_with_model(\n",
    "                ng,\n",
    "                ngram_counts=train_trigram_counts,\n",
    "                lower_order_counts=train_bigram_counts,\n",
    "                V=V,\n",
    "                total_unigram_tokens=total_unigram_tokens,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"Evaluation implemented only for N = 1, 2, 3.\")\n",
    "\n",
    "        total_log_prob += np.log(prob)\n",
    "\n",
    "    avg_log_prob = total_log_prob / max(len(tokens), 1)\n",
    "    return total_log_prob, avg_log_prob\n",
    "\n",
    "\n",
    "def evaluate_ngram_model_on_test(\n",
    "    test_df,\n",
    "    n,\n",
    "    train_unigram_counts,\n",
    "    train_bigram_counts,\n",
    "    train_trigram_counts,\n",
    "    V,\n",
    "    total_unigram_tokens,\n",
    "    alpha=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the N-gram model on the test set:\n",
    "    - Average log-likelihood per token\n",
    "    - Perplexity\n",
    "    \"\"\"\n",
    "    total_log_prob_all = 0.0\n",
    "    total_tokens_all   = 0\n",
    "\n",
    "    for tokens in test_df[\"tokens_final\"]:\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        log_p, _ = sentence_log_probability_with_model(\n",
    "            tokens,\n",
    "            n=n,\n",
    "            train_unigram_counts=train_unigram_counts,\n",
    "            train_bigram_counts=train_bigram_counts,\n",
    "            train_trigram_counts=train_trigram_counts,\n",
    "            V=V,\n",
    "            total_unigram_tokens=total_unigram_tokens,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "\n",
    "        total_log_prob_all += log_p\n",
    "        total_tokens_all   += len(tokens)\n",
    "\n",
    "    avg_log_prob_per_token = total_log_prob_all / total_tokens_all\n",
    "    perplexity = np.exp(-avg_log_prob_per_token)\n",
    "\n",
    "    print(f\"[Evaluation] N = {n} (Laplace smoothing, alpha={alpha})\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total tokens in test set        : {total_tokens_all:,}\")\n",
    "    print(f\"Average log P per token (test)  : {avg_log_prob_per_token:.4f}\")\n",
    "    print(f\"Perplexity on test set          : {perplexity:.4f}\\n\")\n",
    "\n",
    "    return avg_log_prob_per_token, perplexity\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# RUN EVALUATION FOR N = 1, 2, 3\n",
    "# ----------------------------------------------------------\n",
    "evaluation_rows = []\n",
    "for n in [1, 2, 3]:\n",
    "    avg_log, ppl = evaluate_ngram_model_on_test(\n",
    "        test_df=test_df,\n",
    "        n=n,\n",
    "        train_unigram_counts=train_unigram_counts,\n",
    "        train_bigram_counts=train_bigram_counts,\n",
    "        train_trigram_counts=train_trigram_counts,\n",
    "        V=V_train,\n",
    "        total_unigram_tokens=TOTAL_UNIGRAM_TOKENS_TRAIN,\n",
    "        alpha=1.0,   # Laplace smoothing parameter\n",
    "    )\n",
    "    evaluation_rows.append({\n",
    "        \"N\": n,\n",
    "        \"Avg log P per token\": avg_log,\n",
    "        \"Perplexity\": ppl,\n",
    "    })\n",
    "\n",
    "results_52_df = pd.DataFrame(evaluation_rows)\n",
    "display(results_52_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(results_52_df[\"N\"].astype(str), results_52_df[\"Perplexity\"], color=\"#4b9cd3\")\n",
    "ax.set_xlabel(\"N-gram order\")\n",
    "ax.set_ylabel(\"Perplexity (lower is better)\")\n",
    "ax.set_title(\"Test-set perplexity by model order (Laplace α=1.0)\")\n",
    "for idx, value in enumerate(results_52_df[\"Perplexity\"]):\n",
    "    ax.text(idx, value * 1.01, f\"{value:.1f}\", ha=\"center\", va=\"bottom\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c985a3f-0200-4649-9a3f-77b1702464e6",
   "metadata": {},
   "source": [
    "### 5.3 and 5.4\n",
    "5.3 Evaluate the model on the test set. \n",
    "\n",
    "5.4 Implement and compare the performance of different N-gram sizes (e.g., bigram, trigram, 4-gram, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "b7dbc33c-3b02-4f0d-85da-3193580f4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Test Split\n",
      "------------------------------------------------------------\n",
      "Total sentences : 100,000\n",
      "Train sentences : 80,000 (80%)\n",
      "Test sentences  : 20,000  (20%)\n",
      "\n",
      "Building 1-gram model (use_boundaries=False) ...\n",
      "Building 1-gram model (use_boundaries=False) ...\n",
      "Building 2-gram model (use_boundaries=True) ...\n",
      "Building 2-gram model (use_boundaries=True) ...\n",
      "Building 3-gram model (use_boundaries=True) ...\n",
      "Building 3-gram model (use_boundaries=True) ...\n",
      "Building 4-gram model (use_boundaries=True) ...\n",
      "Building 4-gram model (use_boundaries=True) ...\n",
      "\n",
      "N-gram model sizes on training set:\n",
      "------------------------------------------------------------\n",
      "1-gram: 41,437 unique 1-grams\n",
      "2-gram: 612,067 unique 2-grams\n",
      "3-gram: 1,315,466 unique 3-grams\n",
      "4-gram: 1,647,396 unique 4-grams\n",
      "\n",
      "Vocabulary size (train)     : 41,437\n",
      "Total unigram tokens (train): 1,918,710\n",
      "\n",
      "Evaluating 1-gram model on test set (Laplace, alpha=1.0)...\n",
      "\n",
      "N-gram model sizes on training set:\n",
      "------------------------------------------------------------\n",
      "1-gram: 41,437 unique 1-grams\n",
      "2-gram: 612,067 unique 2-grams\n",
      "3-gram: 1,315,466 unique 3-grams\n",
      "4-gram: 1,647,396 unique 4-grams\n",
      "\n",
      "Vocabulary size (train)     : 41,437\n",
      "Total unigram tokens (train): 1,918,710\n",
      "\n",
      "Evaluating 1-gram model on test set (Laplace, alpha=1.0)...\n",
      "  Avg log P per token: -7.4898\n",
      "  Perplexity         : 1789.7763\n",
      "\n",
      "Evaluating 2-gram model on test set (Laplace, alpha=1.0)...\n",
      "  Avg log P per token: -7.4898\n",
      "  Perplexity         : 1789.7763\n",
      "\n",
      "Evaluating 2-gram model on test set (Laplace, alpha=1.0)...\n",
      "  Avg log P per token: -7.8186\n",
      "  Perplexity         : 2486.4977\n",
      "\n",
      "Evaluating 3-gram model on test set (Laplace, alpha=1.0)...\n",
      "  Avg log P per token: -7.8186\n",
      "  Perplexity         : 2486.4977\n",
      "\n",
      "Evaluating 3-gram model on test set (Laplace, alpha=1.0)...\n",
      "  Avg log P per token: -9.6235\n",
      "  Perplexity         : 15115.8736\n",
      "\n",
      "Evaluating 4-gram model on test set (Laplace, alpha=1.0)...\n",
      "  Avg log P per token: -9.6235\n",
      "  Perplexity         : 15115.8736\n",
      "\n",
      "Evaluating 4-gram model on test set (Laplace, alpha=1.0)...\n",
      "  Avg log P per token: -10.1560\n",
      "  Perplexity         : 25744.4221\n",
      "\n",
      "\n",
      "=== N-gram Size Comparison (Laplace smoothing, alpha=1.0) ===\n",
      "  Avg log P per token: -10.1560\n",
      "  Perplexity         : 25744.4221\n",
      "\n",
      "\n",
      "=== N-gram Size Comparison (Laplace smoothing, alpha=1.0) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Avg log P per token</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-7.489846</td>\n",
       "      <td>1789.776264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-7.818630</td>\n",
       "      <td>2486.497699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-9.623501</td>\n",
       "      <td>15115.873644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-10.155973</td>\n",
       "      <td>25744.422091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N  Avg log P per token    Perplexity\n",
       "0  1            -7.489846   1789.776264\n",
       "1  2            -7.818630   2486.497699\n",
       "2  3            -9.623501  15115.873644\n",
       "3  4           -10.155973  25744.422091"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 5.3 IMPLEMENT AND COMPARE DIFFERENT N-GRAM SIZES (N = 1,2,3,4)\n",
    "# ==========================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 1: Train–Test Split (sentence level)\n",
    "# ----------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(df))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split_idx = int(0.8 * len(df))\n",
    "train_idx = indices[:split_idx]\n",
    "test_idx  = indices[split_idx:]\n",
    "\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "test_df  = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Train / Test Split\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total sentences : {len(df):,}\")\n",
    "print(f\"Train sentences : {len(train_df):,} (80%)\")\n",
    "print(f\"Test sentences  : {len(test_df):,}  (20%)\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 2: Build N-gram models for multiple N values\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "N_VALUES = [1, 2, 3, 4]   # you can extend this list if you want (but beware of memory)\n",
    "\n",
    "ngram_models_train = {}   # n -> Counter of that n-gram\n",
    "\n",
    "for n in N_VALUES:\n",
    "    use_boundaries = (n > 1)  # boundaries only for >= bigram\n",
    "    print(f\"Building {n}-gram model (use_boundaries={use_boundaries}) ...\")\n",
    "    ngram_models_train[n] = build_ngram_model(\n",
    "        train_df,\n",
    "        token_column=\"tokens_final\",\n",
    "        n=n,\n",
    "        use_boundaries=use_boundaries,\n",
    "    )\n",
    "\n",
    "print(\"\\nN-gram model sizes on training set:\")\n",
    "print(\"-\" * 60)\n",
    "for n in N_VALUES:\n",
    "    print(f\"{n}-gram: {len(ngram_models_train[n]):,} unique {n}-grams\")\n",
    "\n",
    "# Vocabulary size and total tokens based on unigram model\n",
    "unigram_counts_train = ngram_models_train[1]\n",
    "V_train = len(unigram_counts_train)\n",
    "TOTAL_UNIGRAM_TOKENS_TRAIN = sum(unigram_counts_train.values())\n",
    "\n",
    "print(f\"\\nVocabulary size (train)     : {V_train:,}\")\n",
    "print(f\"Total unigram tokens (train): {TOTAL_UNIGRAM_TOKENS_TRAIN:,}\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Generic Laplace-smoothed probability for any N\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def compute_ngram_probability_generic(\n",
    "    ngram,\n",
    "    n,\n",
    "    ngram_models,\n",
    "    V,\n",
    "    total_unigram_tokens,\n",
    "    alpha=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute Laplace-smoothed probability for a given N-gram of order n.\n",
    "\n",
    "    ngram_models: dict {n: Counter}, containing all orders up to N.\n",
    "    \"\"\"\n",
    "    # Unigram case: P(w) = (count(w) + α) / (total_tokens + α * V)\n",
    "    if n == 1:\n",
    "        unigram_counts = ngram_models[1]\n",
    "        count_w = unigram_counts.get(ngram, 0)\n",
    "        numerator   = count_w + alpha\n",
    "        denominator = total_unigram_tokens + alpha * V\n",
    "        return numerator / denominator\n",
    "\n",
    "    # N > 1: P(w_t | history) = (count(history, w_t) + α) / (count(history) + α * V)\n",
    "    ngram_counts      = ngram_models[n]\n",
    "    lower_order_counts = ngram_models[n - 1]\n",
    "\n",
    "    history      = ngram[:-1]\n",
    "    ngram_count  = ngram_counts.get(ngram, 0)\n",
    "    history_count = lower_order_counts.get(history, 0)\n",
    "\n",
    "    numerator   = ngram_count + alpha\n",
    "    denominator = history_count + alpha * V\n",
    "\n",
    "    if denominator == 0:\n",
    "        # completely unseen history -> uniform fallback\n",
    "        return 1.0 / V\n",
    "\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def sentence_log_probability_generic(\n",
    "    tokens,\n",
    "    n,\n",
    "    ngram_models,\n",
    "    V,\n",
    "    total_unigram_tokens,\n",
    "    alpha=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute log P(sentence) using an N-gram model of order n\n",
    "    with Laplace smoothing.\n",
    "    \"\"\"\n",
    "    tokens_padded = add_sentence_boundaries(tokens, n)\n",
    "    ngrams = generate_ngrams(tokens_padded, n)\n",
    "\n",
    "    total_log_prob = 0.0\n",
    "\n",
    "    for ng in ngrams:\n",
    "        prob = compute_ngram_probability_generic(\n",
    "            ng,\n",
    "            n=n,\n",
    "            ngram_models=ngram_models,\n",
    "            V=V,\n",
    "            total_unigram_tokens=total_unigram_tokens,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "        total_log_prob += np.log(prob)\n",
    "\n",
    "    avg_log_prob = total_log_prob / max(len(tokens), 1)\n",
    "    return total_log_prob, avg_log_prob\n",
    "\n",
    "\n",
    "def evaluate_ngram_order_on_test(\n",
    "    test_df,\n",
    "    n,\n",
    "    ngram_models,\n",
    "    V,\n",
    "    total_unigram_tokens,\n",
    "    alpha=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate N-gram of order n on test set:\n",
    "    - Average log-likelihood per token\n",
    "    - Perplexity\n",
    "    \"\"\"\n",
    "    total_log_prob_all = 0.0\n",
    "    total_tokens_all   = 0\n",
    "\n",
    "    for tokens in test_df[\"tokens_final\"]:\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        log_p, _ = sentence_log_probability_generic(\n",
    "            tokens,\n",
    "            n=n,\n",
    "            ngram_models=ngram_models,\n",
    "            V=V,\n",
    "            total_unigram_tokens=total_unigram_tokens,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "\n",
    "        total_log_prob_all += log_p\n",
    "        total_tokens_all   += len(tokens)\n",
    "\n",
    "    avg_log_prob_per_token = total_log_prob_all / total_tokens_all\n",
    "    perplexity = np.exp(-avg_log_prob_per_token)\n",
    "\n",
    "    return avg_log_prob_per_token, perplexity\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 4: Compare different N values (N = 1,2,3,4)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "results = []\n",
    "\n",
    "for n in N_VALUES:\n",
    "    print(f\"Evaluating {n}-gram model on test set (Laplace, alpha=1.0)...\")\n",
    "    avg_log_p, ppl = evaluate_ngram_order_on_test(\n",
    "        test_df=test_df,\n",
    "        n=n,\n",
    "        ngram_models=ngram_models_train,\n",
    "        V=V_train,\n",
    "        total_unigram_tokens=TOTAL_UNIGRAM_TOKENS_TRAIN,\n",
    "        alpha=1.0,\n",
    "    )\n",
    "    results.append({\n",
    "        \"N\": n,\n",
    "        \"Avg log P per token\": avg_log_p,\n",
    "        \"Perplexity\": ppl\n",
    "    })\n",
    "    print(f\"  Avg log P per token: {avg_log_p:.4f}\")\n",
    "    print(f\"  Perplexity         : {ppl:.4f}\\n\")\n",
    "\n",
    "# Put results into a nice table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== N-gram Size Comparison (Laplace smoothing, alpha=1.0) ===\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccbcd1f-9a47-4876-a029-f02d1e18a925",
   "metadata": {},
   "source": [
    "### 5.4 Other smoothing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "e9c2d8c9-e80d-4de6-b65c-686d930bac17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Add-k] Bigram, alpha = 1.0\n",
      "  Total log P(sentence): -75.1647\n",
      "  Avg log P per word   : -8.3516\n",
      "\n",
      "[Add-k] Bigram, alpha = 0.1\n",
      "  Total log P(sentence): -61.7002\n",
      "  Avg log P per word   : -6.8556\n",
      "\n",
      "[Add-k] Bigram, alpha = 0.01\n",
      "  Total log P(sentence): -50.7341\n",
      "  Avg log P per word   : -5.6371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: same bigram model, different smoothing strengths\n",
    "\n",
    "example_tokens = df[\"tokens_final\"].iloc[0]\n",
    "\n",
    "for alpha in [1.0, 0.1, 0.01]:\n",
    "    total_log_p, avg_log_p = sentence_log_probability(\n",
    "        example_tokens,\n",
    "        n=2,          # bigram\n",
    "        alpha=alpha   # Laplace (1.0) vs Lidstone (0.1, 0.01)\n",
    "    )\n",
    "    print(f\"[Add-k] Bigram, alpha = {alpha}\")\n",
    "    print(f\"  Total log P(sentence): {total_log_p:.4f}\")\n",
    "    print(f\"  Avg log P per word   : {avg_log_p:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "3cef6ae6-d3c1-4a53-8f56-de9580aaf189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good–Turing stats (unigram):\n",
      "------------------------------------------\n",
      "Total tokens (N_total) : 2400354\n",
      "Max count (max_c)      : 152589\n",
      "Types with count=1 (N1): 0\n",
      "p0 (mass for unseen)   : 0.0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# We reuse your existing counts:\n",
    "# - unigram_counts : Counter mapping ('word',) -> count\n",
    "# - V              : len(unigram_counts)\n",
    "# - TOTAL_UNIGRAM_TOKENS : sum(unigram_counts.values())\n",
    "\n",
    "def build_good_turing_stats_unigram(unigram_counts):\n",
    "    \"\"\"\n",
    "    Build Good–Turing statistics for a unigram Counter.\n",
    "\n",
    "    unigram_counts: Counter mapping ('word',) -> count\n",
    "\n",
    "    Returns:\n",
    "    - gt_stats: dict with\n",
    "        - N_total        : total token count\n",
    "        - freq_of_freq   : Counter(c) = # of word types with count c\n",
    "        - c_star         : dict c -> c* (adjusted count)\n",
    "        - p0             : total probability mass for unseen types\n",
    "        - max_c          : maximum observed count\n",
    "    \"\"\"\n",
    "    # Total number of observed tokens\n",
    "    N_total = sum(unigram_counts.values())\n",
    "\n",
    "    # Frequency of frequency: N_c = how many types occurred exactly c times\n",
    "    freq_of_freq = Counter(unigram_counts.values())\n",
    "\n",
    "    max_c = max(unigram_counts.values())\n",
    "\n",
    "    # Adjusted counts: c* = (c+1) * (N_{c+1} / N_c)\n",
    "    c_star = {}\n",
    "    for c in range(1, max_c):\n",
    "        Nc   = freq_of_freq.get(c, 0)       # N_c\n",
    "        Ncp1 = freq_of_freq.get(c + 1, 0)   # N_{c+1}\n",
    "        if Nc > 0 and Ncp1 > 0:\n",
    "            c_star[c] = (c + 1) * (Ncp1 / Nc)\n",
    "\n",
    "    # Good–Turing mass for unseen types: p0 = N1 / N_total\n",
    "    N1 = freq_of_freq.get(1, 0)\n",
    "    p0 = N1 / N_total if N_total > 0 else 0.0\n",
    "\n",
    "    gt_stats = {\n",
    "        \"N_total\": N_total,\n",
    "        \"freq_of_freq\": freq_of_freq,\n",
    "        \"c_star\": c_star,\n",
    "        \"p0\": p0,\n",
    "        \"max_c\": max_c,\n",
    "    }\n",
    "    return gt_stats\n",
    "\n",
    "\n",
    "gt_unigram_stats = build_good_turing_stats_unigram(unigram_counts)\n",
    "\n",
    "print(\"Good–Turing stats (unigram):\")\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Total tokens (N_total) :\", gt_unigram_stats[\"N_total\"])\n",
    "print(\"Max count (max_c)      :\", gt_unigram_stats[\"max_c\"])\n",
    "print(\"Types with count=1 (N1):\", gt_unigram_stats[\"freq_of_freq\"].get(1, 0))\n",
    "print(\"p0 (mass for unseen)   :\", gt_unigram_stats[\"p0\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "05a327ea-4883-4d17-9095-2d83af406ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_turing_unigram_prob(word, unigram_counts, gt_stats, vocab_size=None):\n",
    "    \"\"\"\n",
    "    Compute P(word) under Good–Turing unigram model.\n",
    "\n",
    "    - word is a string, e.g. \"the\"\n",
    "    - unigram_counts has keys like ('the',)\n",
    "    - gt_stats is from build_good_turing_stats_unigram()\n",
    "    - vocab_size: number of types in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    - probability P(word) (float)\n",
    "    \"\"\"\n",
    "    if vocab_size is None:\n",
    "        vocab_size = len(unigram_counts)\n",
    "\n",
    "    # Our Counter uses ('word',) as key\n",
    "    count = unigram_counts.get((word,), 0)\n",
    "\n",
    "    N_total = gt_stats[\"N_total\"]\n",
    "    c_star  = gt_stats[\"c_star\"]\n",
    "    p0      = gt_stats[\"p0\"]    # total prob mass for unseen types\n",
    "    max_c   = gt_stats[\"max_c\"]\n",
    "\n",
    "    # Case 1: seen words (count > 0)\n",
    "    if count > 0:\n",
    "        # If we have an adjusted count c* for this c, use it\n",
    "        if count in c_star:\n",
    "            adjusted_c = c_star[count]\n",
    "            return adjusted_c / N_total\n",
    "        else:\n",
    "            # For very large counts where we didn't compute c*, fallback to MLE\n",
    "            return count / N_total\n",
    "\n",
    "    # Case 2: unseen words (count == 0) – should be rare because of <UNK>\n",
    "    # Distribute p0 uniformly over all unseen types (simple approximation)\n",
    "    # Here we just split over vocab_size as a fallback.\n",
    "    if vocab_size > 0:\n",
    "        return p0 / vocab_size\n",
    "    else:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "12a55366-37e8-4f87-8b0a-9e2632e8ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_prob_unigram_good_turing(tokens, unigram_counts, gt_stats):\n",
    "    \"\"\"\n",
    "    Compute log P(sentence) under a Good–Turing unigram model.\n",
    "\n",
    "    tokens: list of tokens (already lowercased & OOV-handled)\n",
    "    \"\"\"\n",
    "    # For N=1 we used: tokens + </s>  (your add_sentence_boundaries(., 1))\n",
    "    tokens_padded = add_sentence_boundaries(tokens, n=1)\n",
    "    ngrams = generate_ngrams(tokens_padded, 1)   # each is ('word',)\n",
    "\n",
    "    total_log_prob = 0.0\n",
    "    for (w,) in ngrams:\n",
    "        p = good_turing_unigram_prob(\n",
    "            word=w,\n",
    "            unigram_counts=unigram_counts,\n",
    "            gt_stats=gt_stats,\n",
    "            vocab_size=len(unigram_counts)\n",
    "        )\n",
    "        # avoid log(0)\n",
    "        if p <= 0:\n",
    "            # tiny fallback\n",
    "            p = 1.0 / (len(unigram_counts) * 10_000)\n",
    "        total_log_prob += np.log(p)\n",
    "\n",
    "    avg_log_prob = total_log_prob / max(len(tokens), 1)\n",
    "    return total_log_prob, avg_log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "324933b4-76c6-4099-9833-ff19d34d3abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokens: ['1', '!', '03alpe01', 'it', \"'s\", 'a', 'rip', 'off', '.']\n",
      "\n",
      "=== Unigram smoothing comparison ===\n",
      "[Laplace   α=1.0] Total log P: -81.6864, Avg log P/word: -9.0763\n",
      "[Lidstone  α=0.1] Total log P: -84.2634, Avg log P/word: -9.3626\n",
      "[Good–Turing    ] Total log P: -87.6512, Avg log P/word: -9.7390\n"
     ]
    }
   ],
   "source": [
    "# Pick an example sentence (already preprocessed & OOV-handled)\n",
    "example_tokens = df[\"tokens_final\"].iloc[0]\n",
    "print(\"Example tokens:\", example_tokens)\n",
    "\n",
    "# 1) Unigram with Laplace (alpha = 1.0)\n",
    "lap_total, lap_avg = sentence_log_probability(example_tokens, n=1, alpha=1.0)\n",
    "\n",
    "# 2) Unigram with Lidstone (alpha = 0.1)\n",
    "lid_total, lid_avg = sentence_log_probability(example_tokens, n=1, alpha=0.1)\n",
    "\n",
    "# 3) Unigram with Good–Turing\n",
    "gt_total, gt_avg = sentence_log_prob_unigram_good_turing(\n",
    "    example_tokens,\n",
    "    unigram_counts=unigram_counts,\n",
    "    gt_stats=gt_unigram_stats\n",
    ")\n",
    "\n",
    "print(\"\\n=== Unigram smoothing comparison ===\")\n",
    "print(f\"[Laplace   α=1.0] Total log P: {lap_total:.4f}, Avg log P/word: {lap_avg:.4f}\")\n",
    "print(f\"[Lidstone  α=0.1] Total log P: {lid_total:.4f}, Avg log P/word: {lid_avg:.4f}\")\n",
    "print(f\"[Good–Turing    ] Total log P: {gt_total:.4f}, Avg log P/word: {gt_avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbc1b8",
   "metadata": {},
   "source": [
    "### Model Perplexity Comparison Across Smoothing Techniques\n",
    "We evaluate how unigram models behave under multiple smoothing strategies (Laplace, Lidstone, and Good–Turing) by comparing their test-set perplexities side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "37c9cd65-9105-4911-96d6-a7fd63e2d80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluation] N = 1 (Laplace smoothing, alpha=1.0)\n",
      "------------------------------------------------------------\n",
      "Total tokens in test set        : 481,644\n",
      "Average log P per token (test)  : -7.4898\n",
      "Perplexity on test set          : 1789.7763\n",
      "\n",
      "[Evaluation] N = 1 (Laplace smoothing, alpha=0.1)\n",
      "------------------------------------------------------------\n",
      "Total tokens in test set        : 481,644\n",
      "Average log P per token (test)  : -7.5932\n",
      "Perplexity on test set          : 1984.6857\n",
      "\n",
      "[Evaluation] N = 1 (Laplace smoothing, alpha=0.1)\n",
      "------------------------------------------------------------\n",
      "Total tokens in test set        : 481,644\n",
      "Average log P per token (test)  : -7.5932\n",
      "Perplexity on test set          : 1984.6857\n",
      "\n",
      "[Evaluation] N = 1 (Laplace smoothing, alpha=0.01)\n",
      "------------------------------------------------------------\n",
      "Total tokens in test set        : 481,644\n",
      "Average log P per token (test)  : -7.6971\n",
      "Perplexity on test set          : 2202.0615\n",
      "\n",
      "[Evaluation] N = 1 (Laplace smoothing, alpha=0.01)\n",
      "------------------------------------------------------------\n",
      "Total tokens in test set        : 481,644\n",
      "Average log P per token (test)  : -7.6971\n",
      "Perplexity on test set          : 2202.0615\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smoothing</th>\n",
       "      <th>Avg log P per token</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Laplace α=1.0</td>\n",
       "      <td>-7.489846</td>\n",
       "      <td>1789.776264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lidstone α=0.1</td>\n",
       "      <td>-7.593216</td>\n",
       "      <td>1984.685719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lidstone α=0.01</td>\n",
       "      <td>-7.697149</td>\n",
       "      <td>2202.061524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good–Turing</td>\n",
       "      <td>-7.972019</td>\n",
       "      <td>2898.703355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Smoothing  Avg log P per token   Perplexity\n",
       "0    Laplace α=1.0            -7.489846  1789.776264\n",
       "1   Lidstone α=0.1            -7.593216  1984.685719\n",
       "2  Lidstone α=0.01            -7.697149  2202.061524\n",
       "3      Good–Turing            -7.972019  2898.703355"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_good_turing_unigram_on_test(test_df, unigram_counts, gt_stats):\n",
    "    \"\"\"Aggregate Good–Turing unigram log probabilities over the test set.\"\"\"\n",
    "    total_log_prob = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for tokens in test_df[\"tokens_final\"]:\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        log_p, _ = sentence_log_prob_unigram_good_turing(\n",
    "            tokens,\n",
    "            unigram_counts=unigram_counts,\n",
    "            gt_stats=gt_stats,\n",
    "        )\n",
    "        total_log_prob += log_p\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "    avg_log_prob_per_token = total_log_prob / total_tokens\n",
    "    perplexity = np.exp(-avg_log_prob_per_token)\n",
    "    return avg_log_prob_per_token, perplexity\n",
    "\n",
    "\n",
    "smoothing_configs = [\n",
    "    (\"Laplace α=1.0\", 1.0),\n",
    "    (\"Lidstone α=0.1\", 0.1),\n",
    "    (\"Lidstone α=0.01\", 0.01),\n",
    "]\n",
    "\n",
    "smoothing_results = []\n",
    "for label, alpha in smoothing_configs:\n",
    "    avg_log, ppl = evaluate_ngram_model_on_test(\n",
    "        test_df=test_df,\n",
    "        n=1,\n",
    "        train_unigram_counts=train_unigram_counts,\n",
    "        train_bigram_counts=train_bigram_counts,\n",
    "        train_trigram_counts=train_trigram_counts,\n",
    "        V=V_train,\n",
    "        total_unigram_tokens=TOTAL_UNIGRAM_TOKENS_TRAIN,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    smoothing_results.append(\n",
    "        {\"Smoothing\": label, \"Avg log P per token\": avg_log, \"Perplexity\": ppl}\n",
    "    )\n",
    "\n",
    "# Good–Turing handles smoothing differently, so evaluate separately\n",
    "gt_avg_log, gt_ppl = evaluate_good_turing_unigram_on_test(\n",
    "    test_df=test_df,\n",
    "    unigram_counts=train_unigram_counts,\n",
    "    gt_stats=gt_unigram_stats,\n",
    ")\n",
    "smoothing_results.append(\n",
    "    {\"Smoothing\": \"Good–Turing\", \"Avg log P per token\": gt_avg_log, \"Perplexity\": gt_ppl}\n",
    ")\n",
    "\n",
    "smoothing_df = pd.DataFrame(smoothing_results)\n",
    "display(smoothing_df)\n",
    "\n",
    "palette = [\"#4b9cd3\", \"#6ec4db\", \"#b5e48c\", \"#ffb703\"]\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "x_pos = np.arange(len(smoothing_df))\n",
    "ax.bar(x_pos, smoothing_df[\"Perplexity\"], color=palette[: len(smoothing_df)])\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(smoothing_df[\"Smoothing\"], rotation=15, ha=\"right\")\n",
    "ax.set_ylabel(\"Perplexity (lower is better)\")\n",
    "ax.set_title(\"Model Perplexity Comparison Across Smoothing Techniques (Unigram)\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "for x, value in zip(x_pos, smoothing_df[\"Perplexity\"]):\n",
    "    ax.text(x, value * 1.01, f\"{value:.1f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
